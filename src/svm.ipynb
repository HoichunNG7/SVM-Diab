{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission.*\n",
    "\n",
    "In this exercise you will:\n",
    "    \n",
    "- implement the fully-vectorized **loss function** for the linear SVM\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier\n",
    "\n",
    "Your code for this section will be written inside **svm/classifiers/**. \n",
    "\n",
    "Please implement the fully-vectorized **loss function** for the linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(576, 8) (192, 8)\nloss: 1.002500\n"
     ]
    }
   ],
   "source": [
    "from svm.classifiers.linear_svm import linear_svm_loss_vectorized\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "diabetes = pd.read_csv('svm/datasets/diabetes.csv')\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(diabetes.loc[:, diabetes.columns != 'Outcome']), np.array(diabetes['Outcome']), stratify=np.array(diabetes['Outcome']), random_state=66)\n",
    "print(x_train.shape, x_test.shape)\n",
    "# Split the data into train, val, and test sets. In addition we will\n",
    "# create a small development set as a subset of the training data;\n",
    "# we can use this for development so our code runs faster.\n",
    "num_training = 480\n",
    "num_validation = 96 \n",
    "num_test = 160\n",
    "num_dev = 32\n",
    "\n",
    "# Change label set from {0, 1} to {-1, 1}\n",
    "y_train = 2 * y_train - 1\n",
    "y_test = 2 * y_test - 1\n",
    "\n",
    "# Our validation set will be num_validation points from the original\n",
    "# training set.\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = x_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "# Our training set will be the first num_train points from the original\n",
    "# training set.\n",
    "mask = range(num_training)\n",
    "X_train = x_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# We will also make a development set, which is a small subset of\n",
    "# the training set.\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = x_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "# We use the first num_test points of the original test set as our\n",
    "# test set.\n",
    "mask = range(num_test)\n",
    "X_test = x_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "\n",
    " \n",
    "# generate a random SVM weight matrix of small numbers\n",
    "W = np.random.randn(9, ) * 0.0001 \n",
    "#### Evaluate the implementation of the linear SVM loss we provided for you:\n",
    "loss, grad = linear_svm_loss_vectorized(W, X_dev, y_dev, 0.00001)\n",
    "\n",
    "#print scores.shape\n",
    "print('loss: %f' % (loss, ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grad` returned from the function above is right now all zero. Derive and implement the gradient for the SVM cost function and implement it inline inside the function `linear_svm_loss_vectorized`. You will find it helpful to interleave your new code inside the existing function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "We now have vectorized and efficient expressions for the loss, the gradient. We are therefore ready to do SGD to minimize the loss of the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "That took 6.047517s\n"
     ]
    }
   ],
   "source": [
    "# In the file svm_classifier.py, implement SGD in the function\n",
    "# SVMClassifier.train() and then run it with the code below.\n",
    "from svm.classifiers import LinearSVM\n",
    "svm = LinearSVM()\n",
    "tic = time.time()\n",
    "loss_hist = svm.train(X_train, y_train, learning_rate=1e-7, reg=5e4,\n",
    "                      num_iters=1500, verbose=True)\n",
    "\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))\n",
    "# print('loss history: ')\n",
    "# print(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training accuracy: 0.654167\nvalidation accuracy: 0.635417\n"
     ]
    }
   ],
   "source": [
    "# Write the SVMClassifier.predict function and evaluate the performance on both the training and validation set\n",
    "y_train_pred = svm.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "y_val_pred = svm.predict(X_val)\n",
    "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "reg: 5.000000; val: 0.697917\n",
      "reg: 6.000000; val: 0.697917\n",
      "reg: 7.000000; val: 0.666667\n",
      "reg: 8.000000; val: 0.697917\n",
      "reg: 9.000000; val: 0.677083\n",
      "reg: 10.000000; val: 0.697917\n",
      "reg: 11.000000; val: 0.708333\n",
      "reg: 12.000000; val: 0.687500\n",
      "reg: 13.000000; val: 0.677083\n",
      "reg: 14.000000; val: 0.687500\n",
      "best validation accuracy achieved during cross-validation: 0.708333\n",
      "regularization strength when best accuracy occurs: 10.000000\n",
      "learning rate when best accuracy occurs: 0.000100\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths.\n",
    "\n",
    "best_val = -1   # The highest validation accuracy that we have seen so far.\n",
    "best_svm = None # The SVM object that achieved the highest validation rate.\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Write code that chooses the best hyperparameters by tuning on the validation #\n",
    "# set. For each combination of hyperparameters, train an SVM on the            #\n",
    "# training set, compute its accuracy on the training and validation sets.      #\n",
    "# In addition, store the best                                                  #\n",
    "# validation accuracy in best_val and the SVM object that achieves this        #\n",
    "# accuracy in best_svm.                                                        #\n",
    "#                                                                              #\n",
    "# Hint: You should use a small value for num_iters as you develop your         #\n",
    "# validation code so that the SVMs don't take much time to train; once you are #\n",
    "# confident that your validation code works, you should rerun the validation   #\n",
    "# code with a larger value for num_iters.                                      #\n",
    "################################################################################\n",
    "\n",
    "# Grid Search\n",
    "reg_list = [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000]\n",
    "rate_list = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "\n",
    "trn_accuracy_val = -1 # highest validation accuracy for training set\n",
    "\n",
    "best_trn_reg = None\n",
    "best_trn_rate = None\n",
    "best_val_reg = None\n",
    "best_val_rate = None\n",
    "\n",
    "for reg in reg_list:\n",
    "    for rate in rate_list:\n",
    "        svm = LinearSVM()\n",
    "        loss_hist = svm.train(X_train, y_train, learning_rate=rate, reg=reg,\n",
    "                      num_iters=500, verbose=True)\n",
    "\n",
    "        y_train_pred = svm.predict(X_train)\n",
    "        val = np.mean(y_train == y_train_pred)\n",
    "        if val > trn_accuracy_val:\n",
    "            trn_accuracy_val = val\n",
    "            best_trn_reg = reg\n",
    "            best_trn_rate = rate\n",
    "        \n",
    "        y_val_pred = svm.predict(X_val)\n",
    "        val = np.mean(y_val == y_val_pred)\n",
    "        if val > best_val:\n",
    "            best_val = val\n",
    "            best_val_reg = reg\n",
    "            best_val_rate = rate\n",
    "\n",
    "# After a coarse-grained grid search, the highest accuracy occurs normally when reg=10 & learning rate=1e-4. Let's take a finer search on regularization strength.\n",
    "\n",
    "reg_step = best_val_reg / 10\n",
    "\n",
    "for i in range(5, 15):\n",
    "    svm = LinearSVM()\n",
    "    reg = i * reg_step\n",
    "    loss_hist = svm.train(X_train, y_train, learning_rate=best_val_rate, reg=reg,\n",
    "                      num_iters=500, verbose=True)\n",
    "    y_val_pred = svm.predict(X_val)\n",
    "    val = np.mean(y_val == y_val_pred)\n",
    "\n",
    "    #print('reg: %f; val: %f' % (reg, val))\n",
    "    if val > best_val:\n",
    "        best_val = val\n",
    "        best_val_reg = reg\n",
    "        best_svm = svm\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "# Your code\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "print('regularization strength when best accuracy occurs: %f' % best_val_reg)\n",
    "print('learning rate when best accuracy occurs: %f' % best_val_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "validation accuracy achieved on test dataset: 0.693750\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best linear SVM on test set\n",
    "# Your code\n",
    "\n",
    "clf = LinearSVM()\n",
    "loss_hist = clf.train(X_train, y_train, learning_rate=best_val_rate, reg=best_val_reg,\n",
    "                      num_iters=5000, verbose=True)\n",
    "y_pred = clf.predict(X_test)\n",
    "val = np.mean(y_test == y_pred)\n",
    "\n",
    "print('validation accuracy achieved on test dataset: %f' % val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.4 64-bit",
   "display_name": "Python 3.7.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0600588c3b5f4418cbe7b5ebc6825b479f3bc010269d8b60d75058cdd010adfe"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}