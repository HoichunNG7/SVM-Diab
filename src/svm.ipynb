{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission.*\n",
    "\n",
    "In this exercise you will:\n",
    "    \n",
    "- implement the fully-vectorized **loss function** for the linear SVM\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier\n",
    "\n",
    "Your code for this section will be written inside **svm/classifiers/**. \n",
    "\n",
    "Please implement the fully-vectorized **loss function** for the linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(576, 8) (192, 8)\nloss: 1.000522\ny_train(part): \n[ 1 -1  1  1 -1  1 -1 -1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "from svm.classifiers.linear_svm import linear_svm_loss_vectorized\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "diabetes = pd.read_csv('svm/datasets/diabetes.csv')\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(diabetes.loc[:, diabetes.columns != 'Outcome']), np.array(diabetes['Outcome']), stratify=np.array(diabetes['Outcome']), random_state=66)\n",
    "print(x_train.shape, x_test.shape)\n",
    "# Split the data into train, val, and test sets. In addition we will\n",
    "# create a small development set as a subset of the training data;\n",
    "# we can use this for development so our code runs faster.\n",
    "num_training = 480\n",
    "num_validation = 96 \n",
    "num_test = 160\n",
    "num_dev = 32\n",
    "\n",
    "# Change label set from {0, 1} to {-1, 1}\n",
    "y_train = 2 * y_train - 1\n",
    "y_test = 2 * y_test - 1\n",
    "\n",
    "# Our validation set will be num_validation points from the original\n",
    "# training set.\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = x_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "# Our training set will be the first num_train points from the original\n",
    "# training set.\n",
    "mask = range(num_training)\n",
    "X_train = x_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# We will also make a development set, which is a small subset of\n",
    "# the training set.\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = x_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "# We use the first num_test points of the original test set as our\n",
    "# test set.\n",
    "mask = range(num_test)\n",
    "X_test = x_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "\n",
    " \n",
    "# generate a random SVM weight matrix of small numbers\n",
    "W = np.random.randn(9, ) * 0.0001 \n",
    "#### Evaluate the implementation of the linear SVM loss we provided for you:\n",
    "loss, grad = linear_svm_loss_vectorized(W, X_dev, y_dev, 0.00001)\n",
    "\n",
    "#print scores.shape\n",
    "print('loss: %f' % (loss, ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grad` returned from the function above is right now all zero. Derive and implement the gradient for the SVM cost function and implement it inline inside the function `linear_svm_loss_vectorized`. You will find it helpful to interleave your new code inside the existing function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "We now have vectorized and efficient expressions for the loss, the gradient. We are therefore ready to do SGD to minimize the loss of the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "91129, 0.9626569159384498, 0.9595011670982201, 0.9617013725720588, 0.9645827490030213, 0.9612234257809873, 0.9566012940540647, 0.9517684353325011, 0.9689549498966423, 0.9558177738853074, 0.9428387400525803, 0.9585924867337439, 0.9520296473746283, 0.9562440719111345, 0.960264717440944, 0.9566045733702256, 0.95988305399426, 0.9573996991641363, 0.9600814029558059, 0.960917548323429, 0.9582183102267743, 0.9555976262953007, 0.9577468308009852, 0.9549303440000587, 0.9556533142670804, 0.9532393287305239, 0.9621931359649126, 0.9539677159020549, 0.959058417772372, 0.9578315432272281, 0.9524682084021658, 0.9488897501795657, 0.9660176646916203, 0.9550122358949293, 0.961404250988569, 0.9567785830375332, 0.9523450397078922, 0.9515328005457223, 0.9632576631296199, 0.9492007090391479, 0.9555502417071854, 0.9500180990328131, 0.964563634764923, 0.9600646028034506, 0.950310322127368, 0.9577893771787988, 0.9556394937429248, 0.9613673468168834, 0.962706233053054, 0.9519145387399586, 0.9560792048103637, 0.9647396453018168, 0.9613962467222004, 0.955576622295531, 0.9519701987787701, 0.9551170180600035, 0.9577072735523516, 0.9500171885941654, 0.9572780824954179, 0.957765740234822, 0.9510618533115586, 0.9529293241879939, 0.9564555375497262, 0.9568141081383722, 0.9499520751194133, 0.9598571389779778, 0.9491314583361524, 0.9559535113760266, 0.9584837117058354, 0.9511724766313381, 0.9627266221871004, 0.9627130109457545, 0.9590041215976609, 0.9504810460915348, 0.9570302633331477, 0.9559989873942767, 0.9582469607543455, 0.9558043977756244, 0.9606236027802784, 0.9608144784193395, 0.9559525849967545, 0.9598298112653993, 0.9578153454331444, 0.9613079018424375, 0.9594202935352086, 0.9528467226190555, 0.9598233380137806, 0.9515702161652537, 0.9620161021590105, 0.9486150733116834, 0.951746394846518, 0.9524565929852983, 0.9537178478710306, 0.9600874716489372, 0.9616161471537754, 0.9642745169924991, 0.9630385645542344, 0.9557535357093211, 0.9580575591783298, 0.9529965851059857, 0.9519846552838882, 0.9643272704993049, 0.9604425524922455, 0.9603658260938147, 0.9467928120987412, 0.9527531865620361, 0.9614803723668283, 0.9531732665153395, 0.9632814649385644, 0.964996327612448, 0.960653236678107, 0.961543071441923, 0.9621875992403107, 0.9507804318464508, 0.9431857336892563, 0.960048076892207, 0.9530400669568054, 0.9577441861325273, 0.9583419245378102, 0.9585776995685288, 0.9521240720337386, 0.9588845079279751, 0.9620219618697337, 0.9542067164395586, 0.9530188965128105, 0.9619711331311845, 0.9555908202025782, 0.9613979173091797, 0.9508012382655926, 0.9666230149976045, 0.9578061616665022, 0.9606665475132888, 0.9568394936252165, 0.956080834229371, 0.9598458999383367, 0.9618575013070685, 0.9470560006321738, 0.9590460850808679, 0.9593485690314507, 0.9549146586356875, 0.9518860843878809, 0.9585682042165957, 0.9590884645842306, 0.9610702314538684, 0.9583182798573457, 0.9546520754228353, 0.9591334974749082, 0.9563234025877193, 0.958272104722101, 0.9509431685949711, 0.9581804729174381, 0.9558734299485263, 0.9614412673020193, 0.9595690499646153, 0.9542420922117304, 0.9583658818356963, 0.9646246920678431, 0.9517881124928127, 0.9548905429181626, 0.955072333453962, 0.9630492108064702, 0.9651283459287439, 0.959065547658565, 0.964740610744085, 0.9510797420078991, 0.9533929590085383, 0.9594484171276164, 0.9588428739283914, 0.9634399851207759, 0.9603442078397899, 0.9569787711248817, 0.9516428285988456, 0.9580063582907121, 0.9639586990441994, 0.9545717467710776, 0.9546728831786815, 0.9519952799075933, 0.9482039982016292, 0.9596717119731533, 0.9602006863367402, 0.9534131418821749, 0.9661772438537811, 0.9638082302481661, 0.9554245715267122, 0.9548629966481899, 0.9529925295653792, 0.9494790102894788, 0.9578032013153036, 0.9484397203546091, 0.9553499581306807, 0.9674579613412848, 0.9526261388604057, 0.9627400666149138, 0.9560690329852422, 0.9595291403489736, 0.9595529523701647, 0.9578705935241244, 0.9518321505803102, 0.9539287963168371, 0.9553474978937413, 0.9644137892732004, 0.9499822521995057, 0.9586690256447788, 0.9652422603785216, 0.954333619077124, 0.9535042964165021, 0.9531425981749393, 0.9604219524478868, 0.9593889968810762, 0.962020882658063, 0.9568576811594828, 0.9680124838671074, 0.9609892595037998, 0.9576230022462084, 0.9545380404791359, 0.9520219732481863, 0.9566863675896226, 0.952234133819912, 0.9634161780509681, 0.9597458604074329, 0.9632643021951752, 0.9617408505084629, 0.9601012376778959, 0.9645872930164981, 0.9622659614360545, 0.954362839921179, 0.9589561480321536, 0.9604439717708776, 0.9549098637140393, 0.9497848034274269, 0.9585153837475825, 0.9591586167085927, 0.960465986427428, 0.9486223861598164, 0.9509469520874017, 0.9540855767755759, 0.9604188360315922, 0.9541251283594044, 0.9628342571290897, 0.9646637303143415, 0.9556657379692989, 0.9517047617899934, 0.9585022745391624, 0.9592676535733559, 0.9662449164352621, 0.9620630718062746, 0.961482851226207, 0.9535062441793066, 0.949066395621481, 0.9554663165489269, 0.9542111677649462, 0.9539538202666543, 0.9590843450601295, 0.9616217143128211, 0.9607326092412529, 0.952233140745962, 0.9558778245110294, 0.9588567928513585, 0.952366145664606, 0.9589615196018032, 0.9549864765526892, 0.9555986061886812, 0.958628887652814, 0.9595049807062794, 0.9598870789155454, 0.9523396583683915, 0.9616634233600975, 0.9568307423277276, 0.963024869809098, 0.9658418325135237, 0.9674702016524204, 0.9565065604091559, 0.9547822427147572, 0.9511169876895583, 0.9603046616134593, 0.9619993024914507, 0.9577897614057302, 0.9539308577067344, 0.9477956268449782, 0.9538974486084092, 0.9576636224635845, 0.9592622149846113, 0.9560441570369477, 0.9545400384302944, 0.9500345548600765, 0.9508270693393487, 0.9570009036465467, 0.9562887496358329, 0.9631358925306768, 0.9571208723500239, 0.9574142297842231, 0.9644413721338486, 0.952364622743317, 0.9434732626628346, 0.9559764900883243, 0.9532015284141852, 0.9614793270672974, 0.959796818852374, 0.9587879744766196, 0.9476478816245938, 0.9645039604931266, 0.9521371965296156, 0.9529231657242341, 0.9557714876011855, 0.9627496118404092, 0.9623356064198859, 0.9583463506170941, 0.9554430052305158, 0.9562076053374104, 0.9600775038901904, 0.9674433434990454, 0.9618673533044659, 0.9651084200814586, 0.9624602372138888, 0.9555727876933767, 0.9548696508860368, 0.9580561359466268, 0.9598984411434927, 0.9544745689298706, 0.9515249361709928, 0.9621027297525778, 0.9543947548451702, 0.9712185651578449, 0.9647848475311084, 0.9564681914117732, 0.9585291713476569, 0.9550448121566698, 0.9458267778092613, 0.9630042030009257, 0.9581646192290013, 0.9581843819121572, 0.9567760606900996, 0.9657440065192686, 0.9568252443119826, 0.9584608740241705, 0.9641042055810878, 0.9622073539878059, 0.9540367232639307, 0.9509923930741977, 0.9569790919540486, 0.9485681691271033, 0.9517129214318555, 0.9565323050948312, 0.9555864076667611, 0.9591908777067133, 0.9633933670516446, 0.9518525320670405, 0.9576917706380347, 0.9614073397965284, 0.9633675584354406, 0.9496087656490804, 0.9590397132985722, 0.9538679343529592, 0.9557027314154601, 0.9562783405617417, 0.9537771840893728, 0.9491905044106512, 0.9539254180365966, 0.9534082637986933, 0.9676439717502905, 0.9613686634585404, 0.9620280810817705, 0.9535672354558336, 0.9622562113492323, 0.961098643643516, 0.9534547292791258, 0.9536421046480891, 0.9588933755245671, 0.9577640434429856, 0.9525402402242004, 0.9561439537955992, 0.9584643444778216, 0.9593518873158274, 0.96297045144184, 0.9638078876708963, 0.9611420698624161, 0.9560071779583932, 0.9617327169552877, 0.9634759835515723, 0.956541579517196, 0.9555069192152206, 0.9639816988840039, 0.9586391611561812, 0.9567197051213059, 0.9525484265289863, 0.9528247379703982, 0.9659001517298286, 0.9552555271373341, 0.9462460756776047, 0.9540942263687111, 0.967348676539749, 0.9635396151426089, 0.9553254841675944, 0.9585565801018177, 0.9556159359907157, 0.9588607271218521, 0.9605504700704215, 0.960232298626556, 0.9586659663145266, 0.9639670471794046, 0.9540218635866744, 0.9529383850545132, 0.9586375300930321, 0.9614099853921124, 0.9598424573469743, 0.96072159365547, 0.9607629779972778, 0.9634522907127174, 0.9589342673645416, 0.9594175160585579, 0.9564978438987872, 0.9577865355845462, 0.9541681391787273, 0.9575537852548215, 0.9642644180583071, 0.9575134168517222, 0.9602866933687204, 0.9679727416414987, 0.9558439151593403, 0.9715883978870522, 0.955018502900652, 0.9561500688706219, 0.9603769398002788, 0.9566952024375387, 0.9558955787343852, 0.9565870978725151, 0.9556578159377261, 0.956744892212702, 0.9604084388335749, 0.9624888360786819, 0.9488718257502922, 0.954006412145841, 0.9554131504852227, 0.9546609968119045, 0.9627114322075964, 0.9593960403035174, 0.9550016454472404, 0.9638503039573482, 0.9584811596655216, 0.9601469086292856, 0.9603141613628264, 0.9578563000623106, 0.9617320391330394, 0.9536922129330027, 0.9547614780320497, 0.960660537338184, 0.9560937065389987, 0.957015091588683, 0.9577834212667656, 0.9554912037220238, 0.9569611729389113, 0.9557546251663017, 0.963979707585584, 0.9643017956900469, 0.9631114434963757, 0.9661478856761149, 0.9569407584271731, 0.9533030962439352, 0.9616946444269958, 0.9541650308990911, 0.9580923037257604, 0.9643636896837718, 0.955501981061217, 0.9618635412783313, 0.9584016365195461, 0.9560723907529577, 0.9576832870142793, 0.9588768761794509, 0.9543029937451644, 0.9538758646937944, 0.9692408781945775, 0.9628378127032267, 0.9580272931539325, 0.9492448931989625, 0.9593490662142652, 0.9589842574335117, 0.9574670387464747, 0.9583051839754145, 0.9572058109631465, 0.9618052268569092, 0.9616254986023409, 0.9539499570237848, 0.9549488712491707, 0.9582660160101422, 0.9609053574363261, 0.9640273856302167, 0.9490421257030914, 0.962828175582801, 0.9558198868120374, 0.9578432072230798, 0.9577349745976343, 0.9656717292887985, 0.9533719358209694, 0.9548444940650258, 0.960247089008045, 0.9638769477081585, 0.9571906145796691, 0.9672673723279455, 0.9457279053712705, 0.9516852557510214, 0.9639719653238937, 0.9625356029592593, 0.9567293194406996, 0.9627007015972839, 0.9614098224123246, 0.9602871730329214, 0.96361925504552, 0.9600072029175003, 0.9539780196030775, 0.955310815614334, 0.9585718805321172, 0.9672364235002321, 0.9512837695837097, 0.9522997507757894, 0.9629805779807785, 0.95661494601133, 0.9558310822351201, 0.9531937537270555, 0.9515353191545027, 0.9605975751613646, 0.9603738969139721, 0.9541168827703159, 0.9595381488669957, 0.9510380386610434, 0.9549718033552, 0.9583630282508357, 0.9519925320590914, 0.9478740193085757, 0.952534212230916, 0.9662933206377369, 0.958867447326925, 0.9595864184181813, 0.9505085505136489, 0.9496891350844331, 0.9596257835808928, 0.9561283851767455, 0.9604014750766663, 0.9610291569045509, 0.9551442589950185, 0.9553793231987082, 0.9595930034241471, 0.9620058866239838, 0.9596731510713681, 0.9616431677268161, 0.963754056191305, 0.9546010339280033, 0.9500935519503219, 0.9518636278153068, 0.9568636365797434, 0.9448833643969655, 0.9557973483368102, 0.9645981683660502, 0.9604164333353973, 0.9645288510280412, 0.9558393533520657, 0.9650446085632524, 0.9555668285825948, 0.9586407968034182, 0.9572440680830833, 0.9469950684165015, 0.9607181343034322, 0.9522407678738624, 0.9610523670740575, 0.9625548167153566, 0.9584043095304924, 0.961501531323055, 0.9558032653111475, 0.9566922240924236, 0.9538067165636221, 0.9621255207106191, 0.9471563136664589, 0.9576429988463054, 0.9493089426603818, 0.9564527686051463, 0.9598590023916792, 0.9539657118633298, 0.9660618784477536, 0.9601783062577561, 0.9619882340190316, 0.9567931032341669, 0.9578110934125438, 0.9584296524167635, 0.9596263393154278, 0.9477000048215509, 0.9560652029892064, 0.9521531629731264, 0.9580169203412239, 0.966817774488581, 0.9549199085952372, 0.9610952624167782, 0.9600268294788716, 0.9515233574254807, 0.9520856748056141, 0.9556655513373452, 0.9661765666412271, 0.9551590715500881, 0.9500016219834653, 0.9560208444918608, 0.954611339895171, 0.9606887526513629, 0.9547838039082479, 0.9622499526865727, 0.96542295144129, 0.940573726827609, 0.9566097415137856, 0.956285002847094, 0.9622210491096184, 0.946270780951766, 0.9554149095690292, 0.9543095403477547, 0.9661283268910765, 0.9575520908176298, 0.9543975796927866, 0.9561245050630305, 0.9585316556953447, 0.9554567051333873, 0.9596735751255178, 0.9653476245943834, 0.9458917373949958, 0.9553301590103601, 0.9622337146905073, 0.9649044272809941, 0.9588489641417582, 0.9605933404647888, 0.9581973343605666, 0.9538323838528149, 0.9689669713715746, 0.9683232575963284, 0.9610958924953169, 0.9512401950914089, 0.9576513528987073, 0.9647539559133265, 0.95574232697664, 0.9575438715851239, 0.9562660743838586, 0.9487200276949115, 0.9574839019274338, 0.9517433584828471, 0.9609731748656729, 0.955883862270262, 0.9547678064434879, 0.9607376294925243, 0.959470329790533, 0.9630664291009853, 0.9473946513626994, 0.9549578798029518, 0.9588658505891329, 0.9609332122600894, 0.9644796646107376, 0.9557728841336633, 0.959920194096275, 0.9575612836391748, 0.9585072064097193, 0.9660645377996899, 0.9625289138977235, 0.9581353521412957, 0.9508110731669731, 0.965992093361255, 0.9589033206738137, 0.9527662574043052, 0.9481364851010186, 0.9506833813447721, 0.9568759753664356, 0.9606244785144703, 0.9542832290874163, 0.9599042310875148, 0.9539815928466191, 0.9565430819311183, 0.9544295855890111, 0.9588024093874963, 0.9488605493566046, 0.9517327792448522, 0.964332668920199, 0.9618062447957846, 0.958916822052291, 0.9607737011314772, 0.9505253545355541, 0.9577958585898745, 0.9555519512928089, 0.9552960566616009, 0.954045018976996, 0.9597261369514, 0.958611085262606, 0.9554429179872207, 0.9594319987444783, 0.950444145646844, 0.9591232604687466, 0.9627512422520073, 0.9514480114769497, 0.96079482228564, 0.9507606627446472, 0.9589364758019753, 0.948500774054326, 0.9566231857833517, 0.9644429433689901, 0.960701321540427, 0.9559060928376429, 0.9575189776031746, 0.9511941226082189, 0.9551341487357702, 0.9627097823661304, 0.9524108812986476, 0.9621616596085744, 0.9549090532694459, 0.957985794445662, 0.9582972159902478, 0.9588045449652149, 0.9525518579492318, 0.9519140590469459, 0.9504961535807799, 0.9574397292306496, 0.9609270769837619, 0.9539825171286296, 0.9658343866445065, 0.9543708939423167, 0.9593086382694223, 0.9527089747492699, 0.9600589280123516, 0.9572557009534518, 0.9520752694452586, 0.9514175127740372, 0.9603609722287096, 0.955045297982647, 0.9597181243777397, 0.956093861068932, 0.9558419181870177, 0.9563145322850459, 0.9645005285820448, 0.9536588828344371, 0.9602966380719582, 0.9557821878344499, 0.9556956071251193, 0.9500655949818957, 0.9606776908685023, 0.9552957927287156, 0.9511221548097601, 0.9581799003167193, 0.9511257214016999, 0.9613023464422272, 0.9526693386650869, 0.9587237655943872, 0.9547163491155914, 0.9556560793796735, 0.9565202259432695, 0.9456076055593438, 0.9635127279463908, 0.9625413770637187, 0.9530696500836241, 0.9629463705813172, 0.9546875431966184, 0.9613744708720969, 0.9523785868439358, 0.9474790660116926, 0.9627357704537224, 0.9521726592077409, 0.9556883833582013, 0.958005184806709, 0.9578161252822578, 0.9530238746164246, 0.9550355951439253, 0.962281781094789, 0.9490389507308695, 0.9481550279293284, 0.9604582899311112, 0.9571718796867232, 0.9558914036850941, 0.9622337497330778, 0.9494775677076802, 0.9604931649533224, 0.954751904027539, 0.958192321345736, 0.9645084292328376, 0.951861438302071, 0.961265991164041, 0.9513147675184385, 0.9532118766146409, 0.9562920134278137, 0.9554608634153677, 0.9555619173672327, 0.9581400927361508, 0.9551275633170713, 0.9612345939465772, 0.9500498621926728, 0.9623886654764948, 0.9630010992377513, 0.9500314211938949, 0.9598682110305787, 0.9564241823720941, 0.9610802754743358, 0.9549910375325811, 0.9643144308927757, 0.9488386466902533, 0.9658922381157926, 0.9652141169368987, 0.9556543547509402, 0.9650329603573052, 0.9565383089431084, 0.9540720617390894, 0.9642613104740286, 0.9556127815343806, 0.9656017976673903, 0.9489163394240488, 0.9488433346243559, 0.9555709092851966, 0.9501785367406935, 0.9640406235065746, 0.9522993853674399, 0.9647441082918096, 0.9558689223724488, 0.9526377199283099, 0.9611495437692262, 0.9547357861013858, 0.9530610741360913, 0.9583104675618016, 0.945045122493996, 0.9597782314640476, 0.9606612615237909, 0.9613231071069671, 0.9485396049702236, 0.9651283479063746, 0.9590016812341691, 0.954391126448837, 0.9537611869233286, 0.9570187005011683, 0.9631376408625735, 0.9580259990312737, 0.9620490374539732, 0.9559151355826593, 0.9600158533566487, 0.9674210794402789, 0.9558554563251788, 0.961679401492838, 0.9597951276719175, 0.9546003617863614, 0.9589988615099126, 0.9523935300179737, 0.9624480143968388, 0.9580110333646407, 0.9622178963608171, 0.952646691455651, 0.9607351227013184, 0.9551354371785108, 0.9501594951382957, 0.9678632581093843, 0.9625826272126617, 0.9559737059647756, 0.9593891239210574, 0.9504959249611993, 0.9631716288163316, 0.9626345826945691, 0.956887095522487, 0.9552992701815063, 0.9587770242278615, 0.9617226905431455, 0.9456000083614532, 0.9560330822618575, 0.9561906717277053, 0.9656660598132393, 0.9618346215182555, 0.9604656869846683, 0.9635307640204939, 0.9593686708456629, 0.9596579865270486, 0.9531013493727302, 0.9600249445490487, 0.9590818840694345, 0.9657829683181488, 0.9549914092817451, 0.9646372984003322, 0.9578003264251848, 0.957046555876016, 0.956565335029589, 0.9615913825125663, 0.951196618221209, 0.9555449774557025, 0.9543883666711708, 0.958298286140174, 0.9530411081894392, 0.9645767689989462, 0.9572941731853531, 0.9543510654893523, 0.9460182110965759, 0.9542611457022516, 0.9580775615374774, 0.9525027448636165, 0.9610767438682487, 0.9564421446604756, 0.9597929143062409, 0.9631849474338706, 0.9539116730701497, 0.9563727266586629, 0.9606944631156591, 0.9565725122415636, 0.94893972623047, 0.9555008174616018, 0.957852416179255, 0.9604241324904281, 0.956814015890224, 0.9571544265893239, 0.9648708457282837, 0.9608783070677801, 0.9590014568064034, 0.9590766312053929, 0.9582236258796306, 0.9549774439184532, 0.957714139105255, 0.9550786276088585, 0.955756214234169, 0.9614475646025004, 0.9576891982400333, 0.9558580016335562, 0.9585930121853004, 0.9577924610068647, 0.9472440938565615, 0.9550812114967228, 0.9632586721288559, 0.9584703951227656, 0.9552918236127163, 0.9518662116908966, 0.9615549934474568, 0.9610268587625771, 0.958193167297967, 0.9700691910600893, 0.957544373830459, 0.9559505241997102, 0.9605988337927939, 0.9660481601542326, 0.9539652807709936, 0.9549121013179444, 0.9521909293132438, 0.953141749266633, 0.9575060441964192, 0.9516327970334156, 0.9476672337689714, 0.9671612111756506, 0.9593543288000385, 0.958506554640335, 0.9596748383981576, 0.9545057961319884, 0.9667475244605864, 0.9514340772895925, 0.95192885316201, 0.9576855350883475, 0.9606104680711389, 0.9613715118640823, 0.955526586731777, 0.9622720026593563, 0.9577049942356417, 0.9541150576880727, 0.961747704869395, 0.9559996210147297, 0.9576286058134172, 0.9595613553598785, 0.9577092957357045, 0.9587529018972673, 0.9563182656464657, 0.9551252428465049, 0.9616323705270231, 0.9536387994404291, 0.9507734610483543, 0.958114100982576, 0.9555465598812751, 0.9535942741766976, 0.957266132259383, 0.9495144409699607, 0.9519763587321386, 0.9626953260074229, 0.9592726933075281, 0.9547573309898876, 0.9512316906446082, 0.9622936135825075, 0.9612682021286234, 0.9618404403729843, 0.9455008619783336, 0.9517599660364219, 0.961085803828421, 0.9550332913627406, 0.9521581251364596, 0.9479452481506258, 0.9528106864177226, 0.9528357669397617, 0.9547579645162598, 0.9588629873621609, 0.9525645984706679, 0.9403478685350241, 0.9553351748012333, 0.9495214965648527, 0.9605492169742006, 0.9614315509761073, 0.9574455747797379, 0.9576078140258821, 0.9549088695317053, 0.9555395158723518, 0.9645569572349689, 0.9617290351276889, 0.953991478186178, 0.9595025448455252, 0.958441123817786, 0.957743750149461, 0.9496709540035958, 0.9551817197351627, 0.9609839212903156, 0.9612390970318764, 0.9544969348809501, 0.9543979381855091, 0.959000217283214, 0.9675567978053816]\n"
     ]
    }
   ],
   "source": [
    "# In the file svm_classifier.py, implement SGD in the function\n",
    "# SVMClassifier.train() and then run it with the code below.\n",
    "from svm.classifiers import LinearSVM\n",
    "svm = LinearSVM()\n",
    "tic = time.time()\n",
    "loss_hist = svm.train(X_train, y_train, learning_rate=1e-7, reg=5e4,\n",
    "                      num_iters=1500, verbose=True)\n",
    "\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))\n",
    "# print('loss history: ')\n",
    "# print(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.000000\n",
      "validation accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yaozy/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/yaozy/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Write the SVMClassifier.predict function and evaluate the performance on both the training and validation set\n",
    "y_train_pred = svm.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "y_val_pred = svm.predict(X_val)\n",
    "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy achieved during cross-validation: -1.000000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths.\n",
    "\n",
    "best_val = -1   # The highest validation accuracy that we have seen so far.\n",
    "best_svm = None # The SVM object that achieved the highest validation rate.\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Write code that chooses the best hyperparameters by tuning on the validation #\n",
    "# set. For each combination of hyperparameters, train an SVM on the            #\n",
    "# training set, compute its accuracy on the training and validation sets.      #\n",
    "# In addition, store the best                                                  #\n",
    "# validation accuracy in best_val and the SVM object that achieves this        #\n",
    "# accuracy in best_svm.                                                        #\n",
    "#                                                                              #\n",
    "# Hint: You should use a small value for num_iters as you develop your         #\n",
    "# validation code so that the SVMs don't take much time to train; once you are #\n",
    "# confident that your validation code works, you should rerun the validation   #\n",
    "# code with a larger value for num_iters.                                      #\n",
    "################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "# Your code\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best linear SVM on test set\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.4 64-bit",
   "display_name": "Python 3.7.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0600588c3b5f4418cbe7b5ebc6825b479f3bc010269d8b60d75058cdd010adfe"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}